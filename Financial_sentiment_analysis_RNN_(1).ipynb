{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32f3ce8",
      "metadata": {
        "id": "d32f3ce8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "import pandas as pd\n",
        "df= pd.read_csv('data.csv')\n",
        "df.columns= ['Sentence', 'Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca8447d",
      "metadata": {
        "id": "eca8447d",
        "outputId": "f3bc38ae-52ad-4d27-f67d-5383f16e7702"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5837</th>\n",
              "      <td>RISING costs have forced packaging producer Hu...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5838</th>\n",
              "      <td>Nordic Walking was first used as a summer trai...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>According shipping company Viking Line , the E...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>In the building and home improvement trade , s...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5842 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Sentence Sentiment\n",
              "0     The GeoSolutions technology will leverage Bene...  positive\n",
              "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
              "2     For the last quarter of 2010 , Componenta 's n...  positive\n",
              "3     According to the Finnish-Russian Chamber of Co...   neutral\n",
              "4     The Swedish buyout firm has sold its remaining...   neutral\n",
              "...                                                 ...       ...\n",
              "5837  RISING costs have forced packaging producer Hu...  negative\n",
              "5838  Nordic Walking was first used as a summer trai...   neutral\n",
              "5839  According shipping company Viking Line , the E...   neutral\n",
              "5840  In the building and home improvement trade , s...   neutral\n",
              "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive\n",
              "\n",
              "[5842 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0930b933",
      "metadata": {
        "id": "0930b933",
        "outputId": "84922b0b-df8d-42bd-a875-f114e91155e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence     0\n",
              "Sentiment    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f8f7d3",
      "metadata": {
        "id": "46f8f7d3"
      },
      "outputs": [],
      "source": [
        "df=df.dropna(axis='rows')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fcdb98",
      "metadata": {
        "id": "38fcdb98",
        "outputId": "9f689e88-0f32-429f-9aab-d29a3f51c917"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5842, 2)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af44ae6d",
      "metadata": {
        "id": "af44ae6d",
        "outputId": "3e52d505-d04a-4a6a-8bbf-ad0d570b3e87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentiment\n",
              "neutral     3130\n",
              "positive    1852\n",
              "negative     860\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d64046",
      "metadata": {
        "id": "f6d64046",
        "outputId": "fde34da3-8fa3-4aea-c7fb-edb71a07c879"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       positive\n",
              "1       negative\n",
              "2       positive\n",
              "3        neutral\n",
              "4        neutral\n",
              "          ...   \n",
              "5837    negative\n",
              "5838     neutral\n",
              "5839     neutral\n",
              "5840     neutral\n",
              "5841    positive\n",
              "Name: Sentiment, Length: 5842, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val=df.Sentiment\n",
        "val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e350e5a",
      "metadata": {
        "id": "7e350e5a",
        "outputId": "572795c7-ed54-421d-fb58-ff1fa14de75b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 0, 2, ..., 1, 1, 2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "labels= le.fit_transform(val)\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6148654",
      "metadata": {
        "id": "d6148654",
        "outputId": "549d3a48-dd80-4bec-d454-cf49c7d9f2f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences= df.Sentence\n",
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6a3888",
      "metadata": {
        "id": "ca6a3888",
        "outputId": "4fcdc450-8684-44c0-fab0-97abc9aa2854"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5842"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "clean_sentences = [sentence for sentence in sentences ]\n",
        "len(clean_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5bf3ac7",
      "metadata": {
        "id": "f5bf3ac7"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(clean_sentences)\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129934e2",
      "metadata": {
        "id": "129934e2",
        "outputId": "ed5c0471-3192-47fd-dc65-6cd2a4d8220e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5842, 71)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Convert sentences to sequences of indices\n",
        "sequences = tokenizer.texts_to_sequences(clean_sentences)\n",
        "\n",
        "# Pad sequences to ensure they have the same length\n",
        "maxlen = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "padded_sequences.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e62c7d7",
      "metadata": {
        "id": "3e62c7d7",
        "outputId": "1ebffc1b-8585-4085-daed-f8a22a39d72a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5842,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4053a00",
      "metadata": {
        "id": "d4053a00",
        "outputId": "af3fcfe6-433a-4116-e6c2-71516726811b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dce6e1",
      "metadata": {
        "id": "78dce6e1",
        "outputId": "f14a9eb0-2195-46a3-9138-5a5dbc786446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91720\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.5006 - loss: 1.1156\n",
            "Epoch 2/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6594 - loss: 0.8198\n",
            "Epoch 3/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.7982 - loss: 0.5409\n",
            "Epoch 4/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8523 - loss: 0.3654\n",
            "Epoch 5/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.8877 - loss: 0.2696\n",
            "Epoch 6/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8962 - loss: 0.2067\n",
            "Epoch 7/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8965 - loss: 0.1906\n",
            "Epoch 8/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8949 - loss: 0.1761\n",
            "Epoch 9/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8997 - loss: 0.1595\n",
            "Epoch 10/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9107 - loss: 0.1520\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
            "Sentiment prediction: 2\n"
          ]
        }
      ],
      "source": [
        "# Define RNN model\n",
        "embedding_dim = 16\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Define RNN model for multi-class classification\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    SimpleRNN(32),  # Simple RNN layer with 32 units\n",
        "    Dense(4, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, labels, epochs=10, verbose=1)\n",
        "\n",
        "# Test the model\n",
        "test_sentence = 'This movie is amazing'\n",
        "test_sequence = tokenizer.texts_to_sequences([test_sentence])\n",
        "padded_test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
        "prediction = model.predict(padded_test_sequence)\n",
        "predicted_class = np.argmax(prediction)\n",
        "print(\"Sentiment prediction:\", predicted_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca19b09",
      "metadata": {
        "id": "fca19b09",
        "outputId": "63ba111c-5927-49d2-a926-310ea928558b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 554ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr   # library used for\n",
        "\n",
        "\n",
        "# web interface for project\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_sentiment(text):\n",
        "    # Preprocess the input text\n",
        "    test_sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
        "    # Predict sentiment using the model\n",
        "    prediction = model.predict(padded_test_sequence)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Use LabelEncoder to map predicted class index to original sentiment label\n",
        "    predicted_label = le.inverse_transform([predicted_class])[0]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(fn=predict_sentiment,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"text\",\n",
        "                     title=\"Sentiment Analysis\",\n",
        "                     description=\"Enter a text to predict its sentiment (negative, somewhat negative, somewhat positive, or positive).\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553f1a4e",
      "metadata": {
        "id": "553f1a4e"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    # Preprocess the input text\n",
        "    test_sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
        "    # Predict sentiment using the model\n",
        "    prediction = model.predict(padded_test_sequence)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Use LabelEncoder to map predicted class index to original sentiment label\n",
        "    predicted_label = le.inverse_transform([predicted_class])[0]\n",
        "\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b468ac8",
      "metadata": {
        "id": "2b468ac8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77028a95",
      "metadata": {
        "id": "77028a95",
        "outputId": "9fd141c3-7929-45af-b9e6-78429ba20771"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\91720\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.layers import LSTM, Dropout\n",
        "\n",
        "# Define a more complex LSTM model for multi-class classification\n",
        "model_complex_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "    LSTM(64, return_sequences=True),  # First LSTM layer with 64 units and return sequences\n",
        "    Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "    LSTM(32),  # Second LSTM layer with 32 units\n",
        "    Dropout(0.5),  # Dropout layer\n",
        "    Dense(64, activation='relu'),  # Dense layer with ReLU activation\n",
        "    Dense(4, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5796ee0",
      "metadata": {
        "id": "d5796ee0"
      },
      "outputs": [],
      "source": [
        "model_complex_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d85fc14",
      "metadata": {
        "id": "0d85fc14",
        "outputId": "e2674d69-c193-42e7-9ced-b1b4f7fb7544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 76ms/step - accuracy: 0.5021 - loss: 1.1007\n",
            "Epoch 2/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 82ms/step - accuracy: 0.5310 - loss: 0.9965\n",
            "Epoch 3/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.6450 - loss: 0.8253\n",
            "Epoch 4/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 77ms/step - accuracy: 0.7597 - loss: 0.6123\n",
            "Epoch 5/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 90ms/step - accuracy: 0.8204 - loss: 0.4419\n",
            "Epoch 6/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 77ms/step - accuracy: 0.8728 - loss: 0.3024\n",
            "Epoch 7/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 77ms/step - accuracy: 0.8924 - loss: 0.2324\n",
            "Epoch 8/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 88ms/step - accuracy: 0.8996 - loss: 0.2084\n",
            "Epoch 9/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 71ms/step - accuracy: 0.9001 - loss: 0.1810\n",
            "Epoch 10/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.9036 - loss: 0.1751\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1eac87e9550>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_complex_lstm.fit(padded_sequences, labels, epochs=10, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f1408e",
      "metadata": {
        "id": "90f1408e",
        "outputId": "46cd8603-4c47-4087-80e3-eebde6c2bfc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Sentiment prediction using complex LSTM model: 1\n"
          ]
        }
      ],
      "source": [
        "# Test the complex LSTM model\n",
        "test_sentence = 'This movie is amazing'\n",
        "test_sequence = tokenizer.texts_to_sequences([test_sentence])\n",
        "padded_test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
        "prediction_complex_lstm = model_complex_lstm.predict(padded_test_sequence)\n",
        "predicted_class_complex_lstm = np.argmax(prediction_complex_lstm)\n",
        "print(\"Sentiment prediction using complex LSTM model:\", predicted_class_complex_lstm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1dd0a9",
      "metadata": {
        "id": "0a1dd0a9",
        "outputId": "4f2134b5-a6ca-486a-ac6b-a162bec69a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_sentiment(text):\n",
        "    # Preprocess the input text\n",
        "    test_sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
        "    # Predict sentiment using the model\n",
        "    prediction = model_complex_lstm.predict(padded_test_sequence)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Use LabelEncoder to map predicted class index to original sentiment label\n",
        "    predicted_label = le.inverse_transform([predicted_class])[0]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(fn=predict_sentiment,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"text\",\n",
        "                     title=\"Sentiment Analysis\",\n",
        "                     description=\"Enter a text to predict its sentiment (negative, somewhat negative, somewhat positive, or positive).\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d95ed89",
      "metadata": {
        "id": "6d95ed89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31edb155",
      "metadata": {
        "id": "31edb155"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}